#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTMLæ ‡ç­¾é‡å¤æ£€æŸ¥å·¥å…·
æ£€æŸ¥æ‰€æœ‰HTMLæ–‡ä»¶ä¸­çš„é‡å¤metaæ ‡ç­¾ã€è¯­æ³•é”™è¯¯ç­‰é—®é¢˜
"""

import os
import re
import glob
from collections import Counter
from typing import List, Dict, Tuple

class HTMLDuplicateChecker:
    def __init__(self, directory: str = "."):
        self.directory = directory
        self.issues = []
        
    def find_html_files(self) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶"""
        pattern = os.path.join(self.directory, "*.html")
        return glob.glob(pattern)
    
    def check_duplicate_meta_tags(self, content: str, filename: str) -> List[str]:
        """æ£€æŸ¥é‡å¤çš„metaæ ‡ç­¾"""
        issues = []
        
        # æ£€æŸ¥Open Graphæ ‡ç­¾
        og_patterns = [
            r'<meta\s+property="og:type"[^>]*>',
            r'<meta\s+property="og:url"[^>]*>',
            r'<meta\s+property="og:title"[^>]*>',
            r'<meta\s+property="og:description"[^>]*>',
            r'<meta\s+property="og:image"[^>]*>'
        ]
        
        for pattern in og_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if len(matches) > 1:
                tag_type = re.search(r'og:(\w+)', pattern).group(1)
                issues.append(f"é‡å¤çš„Open Graphæ ‡ç­¾ 'og:{tag_type}': æ‰¾åˆ° {len(matches)} ä¸ª")
        
        # æ£€æŸ¥Twitter Cardæ ‡ç­¾
        twitter_patterns = [
            r'<meta\s+property="twitter:card"[^>]*>',
            r'<meta\s+property="twitter:url"[^>]*>',
            r'<meta\s+property="twitter:title"[^>]*>',
            r'<meta\s+property="twitter:description"[^>]*>',
            r'<meta\s+property="twitter:image"[^>]*>'
        ]
        
        for pattern in twitter_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if len(matches) > 1:
                tag_type = re.search(r'twitter:(\w+)', pattern).group(1)
                issues.append(f"é‡å¤çš„Twitter Cardæ ‡ç­¾ 'twitter:{tag_type}': æ‰¾åˆ° {len(matches)} ä¸ª")
        
        # æ£€æŸ¥canonicalæ ‡ç­¾
        canonical_matches = re.findall(r'<link\s+rel="canonical"[^>]*>', content, re.IGNORECASE)
        if len(canonical_matches) > 1:
            issues.append(f"é‡å¤çš„canonicalæ ‡ç­¾: æ‰¾åˆ° {len(canonical_matches)} ä¸ª")
        
        # æ£€æŸ¥åŸºæœ¬metaæ ‡ç­¾
        meta_patterns = [
            (r'<meta\s+name="description"[^>]*>', "description"),
            (r'<meta\s+name="keywords"[^>]*>', "keywords"),
            (r'<meta\s+name="author"[^>]*>', "author"),
            (r'<meta\s+name="robots"[^>]*>', "robots")
        ]
        
        for pattern, tag_name in meta_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if len(matches) > 1:
                issues.append(f"é‡å¤çš„metaæ ‡ç­¾ '{tag_name}': æ‰¾åˆ° {len(matches)} ä¸ª")
        
        return issues
    
    def check_syntax_errors(self, content: str, filename: str) -> List[str]:
        """æ£€æŸ¥è¯­æ³•é”™è¯¯"""
        issues = []
        
        # æ£€æŸ¥å¤šä½™çš„å°–æ‹¬å·
        extra_brackets = re.findall(r'>>', content)
        if extra_brackets:
            issues.append(f"å‘ç°å¤šä½™çš„å°–æ‹¬å· '>>': {len(extra_brackets)} å¤„")
        
        # æ£€æŸ¥æœªé—­åˆçš„æ ‡ç­¾ (ç®€å•æ£€æŸ¥)
        open_tags = re.findall(r'<(\w+)[^>]*(?<!/)>', content)
        close_tags = re.findall(r'</(\w+)>', content)
        
        # æ’é™¤è‡ªé—­åˆæ ‡ç­¾
        self_closing = ['meta', 'link', 'img', 'br', 'hr', 'input', 'area', 'base', 'col', 'embed', 'source', 'track', 'wbr']
        open_tags = [tag for tag in open_tags if tag.lower() not in self_closing]
        
        open_counter = Counter(open_tags)
        close_counter = Counter(close_tags)
        
        for tag, count in open_counter.items():
            if close_counter.get(tag, 0) != count:
                diff = count - close_counter.get(tag, 0)
                if diff > 0:
                    issues.append(f"å¯èƒ½æœªé—­åˆçš„æ ‡ç­¾ '<{tag}>': å¤šäº† {diff} ä¸ªå¼€æ”¾æ ‡ç­¾")
        
        # æ£€æŸ¥titleæ ‡ç­¾æ ¼å¼é—®é¢˜
        title_matches = re.findall(r'<title>(.*?)</title>', content, re.IGNORECASE)
        for title in title_matches:
            if 'unblockedunblocked' in title.lower():
                issues.append(f"æ ‡é¢˜æ ¼å¼é”™è¯¯: '{title}' (åŒ…å«'unblockedunblocked')")
            if title.count('unblocked') > 2:
                issues.append(f"æ ‡é¢˜ä¸­'unblocked'é‡å¤è¿‡å¤š: '{title}'")
        
        return issues
    
    def check_content_structure(self, content: str, filename: str) -> List[str]:
        """æ£€æŸ¥å†…å®¹ç»“æ„é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥ç©ºçš„aboutéƒ¨åˆ†
        about_pattern = r'<section[^>]*id="about"[^>]*>.*?</section>'
        about_match = re.search(about_pattern, content, re.DOTALL | re.IGNORECASE)
        if about_match:
            about_content = about_match.group(0)
            # ç§»é™¤HTMLæ ‡ç­¾ï¼Œæ£€æŸ¥å®é™…æ–‡æœ¬å†…å®¹
            text_content = re.sub(r'<[^>]+>', '', about_content).strip()
            if len(text_content) < 100:  # å¦‚æœå†…å®¹å°‘äº100å­—ç¬¦
                issues.append("aboutéƒ¨åˆ†å†…å®¹è¿‡å°‘æˆ–ä¸ºç©º")
        
        # æ£€æŸ¥é‡å¤çš„è„šæœ¬å¼•ç”¨
        script_srcs = re.findall(r'<script[^>]*src="([^"]*)"', content, re.IGNORECASE)
        script_counter = Counter(script_srcs)
        for src, count in script_counter.items():
            if count > 1:
                issues.append(f"é‡å¤çš„è„šæœ¬å¼•ç”¨: '{src}' å‡ºç° {count} æ¬¡")
        
        # æ£€æŸ¥é‡å¤çš„CSSå¼•ç”¨
        css_hrefs = re.findall(r'<link[^>]*href="([^"]*\.css)"', content, re.IGNORECASE)
        css_counter = Counter(css_hrefs)
        for href, count in css_counter.items():
            if count > 1:
                issues.append(f"é‡å¤çš„CSSå¼•ç”¨: '{href}' å‡ºç° {count} æ¬¡")
        
        return issues
    
    def check_file(self, filepath: str) -> Dict[str, List[str]]:
        """æ£€æŸ¥å•ä¸ªHTMLæ–‡ä»¶"""
        filename = os.path.basename(filepath)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            try:
                with open(filepath, 'r', encoding='gbk') as f:
                    content = f.read()
            except:
                return {filename: ["æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œæ— æ³•è¯»å–"]}
        except Exception as e:
            return {filename: [f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"]}
        
        all_issues = []
        
        # æ£€æŸ¥å„ç§é—®é¢˜
        all_issues.extend(self.check_duplicate_meta_tags(content, filename))
        all_issues.extend(self.check_syntax_errors(content, filename))
        all_issues.extend(self.check_content_structure(content, filename))
        
        return {filename: all_issues}
    
    def run_check(self) -> Dict[str, List[str]]:
        """è¿è¡Œå®Œæ•´æ£€æŸ¥"""
        html_files = self.find_html_files()
        print(f"ğŸ” æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        print("=" * 60)
        
        all_results = {}
        files_with_issues = 0
        total_issues = 0
        
        for filepath in html_files:
            filename = os.path.basename(filepath)
            print(f"æ£€æŸ¥: {filename}...", end=" ")
            
            result = self.check_file(filepath)
            issues = result[filename]
            
            if issues:
                print(f"âŒ å‘ç° {len(issues)} ä¸ªé—®é¢˜")
                files_with_issues += 1
                total_issues += len(issues)
                all_results.update(result)
            else:
                print("âœ… æ— é—®é¢˜")
        
        print("=" * 60)
        print(f"ğŸ“Š æ£€æŸ¥å®Œæˆ:")
        print(f"   - æ€»æ–‡ä»¶æ•°: {len(html_files)}")
        print(f"   - æœ‰é—®é¢˜çš„æ–‡ä»¶: {files_with_issues}")
        print(f"   - æ€»é—®é¢˜æ•°: {total_issues}")
        
        return all_results
    
    def generate_report(self, results: Dict[str, List[str]]):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        if not results:
            print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰HTMLæ–‡ä»¶éƒ½æ²¡æœ‰å‘ç°é—®é¢˜ï¼")
            return
        
        print(f"\nğŸ“‹ è¯¦ç»†é—®é¢˜æŠ¥å‘Š:")
        print("=" * 60)
        
        for filename, issues in results.items():
            print(f"\nğŸ“„ {filename}")
            print("-" * 40)
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. {issue}")
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        print(f"\nğŸ“ˆ é—®é¢˜ç±»å‹ç»Ÿè®¡:")
        print("-" * 40)
        
        issue_types = {}
        for issues in results.values():
            for issue in issues:
                issue_type = issue.split(':')[0]
                issue_types[issue_type] = issue_types.get(issue_type, 0) + 1
        
        for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {issue_type}: {count} æ¬¡")

def main():
    print("ğŸ”§ HTMLæ ‡ç­¾é‡å¤æ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    checker = HTMLDuplicateChecker()
    results = checker.run_check()
    checker.generate_report(results)
    
    # ç”Ÿæˆä¿®å¤å»ºè®®
    if results:
        print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        print("-" * 40)
        print("1. å¯¹äºé‡å¤çš„metaæ ‡ç­¾ï¼Œåˆ é™¤å¤šä½™çš„é‡å¤é¡¹")
        print("2. å¯¹äºè¯­æ³•é”™è¯¯ï¼Œä¿®æ­£HTMLæ ¼å¼")
        print("3. å¯¹äºç©ºçš„aboutéƒ¨åˆ†ï¼Œæ·»åŠ ä¸°å¯Œçš„å†…å®¹")
        print("4. å¯¹äºé‡å¤çš„èµ„æºå¼•ç”¨ï¼Œåˆå¹¶æˆ–åˆ é™¤é‡å¤é¡¹")
        print("\nä½¿ç”¨ replace_string_in_file å·¥å…·æ¥ä¿®å¤è¿™äº›é—®é¢˜ã€‚")

if __name__ == "__main__":
    main()
